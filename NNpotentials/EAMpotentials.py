import tensorflow as _tf
from .core import nn_layer
import numpy as _np
from itertools import combinations_with_replacement

precision = _tf.float32

def poly_cutoff(input_tensor, cut_a, cut_b):
    with _tf.name_scope("PolyCutoff"):
        return (1.0 - 10.0 * ((input_tensor-cut_a)/(cut_b-cut_a))**3 +
            15.0 * ((input_tensor-cut_a)/(cut_b-cut_a))**4 -
            6.0 * ((input_tensor-cut_a)/(cut_b-cut_a))**5) * \
            _tf.cast(input_tensor < cut_b, dtype = precision) * \
            _tf.cast(input_tensor > cut_a, dtype = precision) +\
            1.0 * _tf.cast(input_tensor < cut_a, dtype = precision)

class EAMpotential():
    def __init__(self, atom_types, error_scaling = 1000):
        self.target = _tf.placeholder(shape = (None,), dtype = precision,
            name = "target")
        self.atom_types = atom_types
        self.error_scaling = error_scaling

        self.ANNs = {}
        self.atom_maps = {}

        for t in self.atom_types:
            self.atom_maps[t] = _tf.sparse_placeholder(shape = (None, None),
                dtype = precision, name = "{}_map".format(t))

    def _post_setup(self):
        self.E_predict = _tf.reduce_sum([
            _tf.sparse_tensor_dense_matmul(self.atom_maps[t],
            self.ANNs[t].output) for t in self.atom_types], axis = [0, 2],
            name = "E_prediction")

        # Tensorflow operation to initialize the variables of the atomic networks
        #self.init_vars = [a.init_vars for a in self.ANNs.itervalues()]

        self.num_atoms =  _tf.reduce_sum(
            [_tf.sparse_reduce_sum(m, axis = 1) for m in self.atom_maps.itervalues()],
            axis = 0, name = "NumberOfAtoms")
        # Tensorflow operation that calculates the sum squared error per atom.
        # Note that the whole error per atom is squared.
        with _tf.name_scope("RMSE"):
            self.rmse_weights = _tf.placeholder(shape = (None,),
                dtype = precision, name = "weights")
            self.rmse = self.error_scaling*_tf.sqrt(_tf.reduce_mean(
                (self.target-self.E_predict)**2*self.rmse_weights))
            #self.rmse = self.error_scaling*_tf.sqrt(
            #    _tf.losses.mean_squared_error(self.target,
            #    self.E_predict, weights = 1.0/self.num_atoms**2))
            self.rmse_summ = _tf.summary.scalar("RMSE", self.rmse, family = "performance")

        self.variables = _tf.get_collection(_tf.GraphKeys.MODEL_VARIABLES,
            scope = _tf.get_default_graph().get_name_scope())
        self.saver = _tf.train.Saver(self.variables, max_to_keep = None, save_relative_paths = True)

    def export(self, sess, file_name, atomic_prop_dict, Nrho = 10000,
        drho = 0.03, Nr = 10000, dr = None, cutoff = 8.2):
        if dr == None:
            dr = cutoff/Nr
        with open(file_name + ".eam.fs", "w") as fout:
            fout.write("Autogenerated potential\n")
            fout.write("Exported on \n")
            fout.write("###########################\n")
            fout.write("".join(["%s"%len(self.atom_types)] +
                ["  %s"%t for t in self.atom_types] + ["\n"]))
            fout.write("{:d}  {:.14E}  {:d}  {:.14E}  {:.14E}\n".format(Nrho, drho, Nr, dr, cutoff))

            rho_vec = _np.arange(Nrho).reshape((-1, 1))*drho
            r_vec = _np.arange(Nr).reshape((-1, 1))*dr
            for t in self.atom_types:
                fout.write("{}  {}  {}  {}\n".format(*atomic_prop_dict[t]))
                F_tab = sess.run(self.ANNs[t].F_out,
                    {self.ANNs[t].sum_rho: rho_vec}).flatten()
                for i in range(0, len(F_tab), 5):
                    # There should be a better way instead of list()
                    fout.write("{:.14E}  {:.14E}  {:.14E}  {:.14E}  {:.14E}\n".format(
                        *list(F_tab[i:(i+5)])))
                for t2 in self.atom_types:
                    rho_tab = sess.run(self.ANNs[t].rho[t2],
                        {self.ANNs[t].inputs[t2]: r_vec}).flatten()
                    for i in range(0, len(rho_tab), 5):
                        # There should be a better way instead of list()
                        fout.write("{:.14E}  {:.14E}  {:.14E}  {:.14E}  {:.14E}\n".format(
                            *list(rho_tab[i:(i+5)])))
            # Following http://lammps.sandia.gov/doc/pair_eam.html only i >= j
            for i, t1 in enumerate(self.atom_types):
                for j, t2 in enumerate(self.atom_types):
                    if i >= j:
                        # For debugging:
                        #fout.write("{}-{} Pair pot (i = {}, j = {})\n".format(t1, t2, i, j))
                        phi_tab = 2*(sess.run(self.ANNs[t1].pairPot[t2],
                            {self.ANNs[t1].inputs[t2]: r_vec})*r_vec).flatten()
                        for k in range(0, len(phi_tab), 5):
                            # There should be a better way instead of list()
                            fout.write("{:.14E}  {:.14E}  {:.14E}  {:.14E}  {:.14E}\n".format(
                                *list(phi_tab[k:(k+5)])))

class EAMAtomicNN():
    def __init__(self, atom_types, offset = 0.0, name = "ANN"):
        self.atom_types = atom_types
        self.name = name

        self.pairPot = {}
        self.F = _tf.identity
        self.rho = {}

        self.inputs = {}
        self.b_maps = {}
        for t in atom_types:
            self.inputs[t] = _tf.placeholder(shape = (None, 1),
                dtype = precision, name = "ANN_input_{}".format(t))
            self.b_maps[t] = _tf.sparse_placeholder(
                dtype = precision, name = "b_map_{}".format(t))
        self.offset = _tf.Variable(offset, dtype = precision, name = "offset",
            collections = [_tf.GraphKeys.MODEL_VARIABLES, _tf.GraphKeys.GLOBAL_VARIABLES])
        _tf.summary.scalar("offset", self.offset, family = "modelParams")

    def _post_setup(self):
        self.sum_rho = _tf.reduce_sum(
            [_tf.sparse_tensor_dense_matmul(self.b_maps[t], self.rho[t])
            for t in self.atom_types], axis = 0, name = "SumRho")
        _tf.summary.histogram("SumRho", self.sum_rho)
        with _tf.variable_scope(self.name+"_EmbeddingFunc", reuse = _tf.AUTO_REUSE):
            self.F_out = self.F(self.sum_rho)
        self.output = _tf.add(_tf.reduce_sum(
                [_tf.sparse_tensor_dense_matmul(self.b_maps[t], self.pairPot[t])
                for t in self.atom_types], axis = 0, name = "SumPairPot") + \
            self.F_out, self.offset, name = "AtomicEnergy")



class SMATBpotential(EAMpotential):
    def __init__(self, atom_types, initial_params = None, offsets = None,
        cut_a = 5.4, cut_b = 8.1, pair_trainable = True, rho_trainable = True,
        r0_trainable = False):
        with _tf.variable_scope("SMATB"):
            EAMpotential.__init__(self, atom_types)

            A = {}
            xi = {}
            p = {}
            q = {}
            r0 = {}
            pairPot = {}
            rho = {}
            for t1, t2 in combinations_with_replacement(atom_types, r = 2):
                t12 = tuple(sorted([t1, t2]))
                A[t12], p[t12], r0[t12], pairPot[t12] = SMATBpotential.build_pairPot(
                    t12, initial_params, cut_a, cut_b, r0_trainable, pair_trainable)
                xi[t12], q[t12], _, rho[t12] = SMATBpotential.build_rho(
                    t12, initial_params, cut_a, cut_b, r0_trainable, rho_trainable)

            if offsets == None:
                offsets = [0.0]*len(self.atom_types)

            for t1, offset in zip(self.atom_types, offsets):
                with _tf.name_scope("{}_ANN".format(t1)):
                    self.ANNs[t1] = EAMAtomicNN(atom_types, offset)
                    self.ANNs[t1].F = lambda rho: -_tf.sqrt(rho)

                    for t2 in self.atom_types:
                        t12 = tuple(sorted([t1, t2]))
                        with _tf.variable_scope("{}_{}_PairPot".format(*t12), reuse = _tf.AUTO_REUSE):
                            self.ANNs[t1].pairPot[t2] = pairPot[t12](
                                self.ANNs[t1].inputs[t2])
                        with _tf.variable_scope("{}_{}_rho".format(*t12), reuse = _tf.AUTO_REUSE):
                            self.ANNs[t1].rho[t2] = rho[t12](
                                self.ANNs[t1].inputs[t2])

                    self.ANNs[t1]._post_setup()
            EAMpotential._post_setup(self)

    @staticmethod
    def build_pairPot(t12, initial_params, cut_a, cut_b, r0_trainable, pair_trainable):
        """ builds the pair potential for a given atom type tuple
            t12 = (t1, t2)
            returns: parameters A, p, r0 and the corresponding pair potential as
                    a function of r
        """
        with _tf.variable_scope("{}_{}_PairPot".format(*t12), reuse = _tf.AUTO_REUSE):
            if ("A", t12[0], t12[1]) in initial_params:
                A_init = initial_params[("A", t12[0], t12[1])]
            else:
                A_init = 0.2
            A = _tf.get_variable("A_{}_{}".format(*t12), dtype = precision,
                initializer = _tf.constant(A_init, dtype = precision),
                trainable = pair_trainable,
                collections = [_tf.GraphKeys.MODEL_VARIABLES,
                                _tf.GraphKeys.GLOBAL_VARIABLES])
            _tf.summary.scalar("A_{}_{}".format(*t12), A, family = "modelParams")

            if ("p", t12[0], t12[1]) in initial_params:
                p_init = initial_params[("p", t12[0], t12[1])]
            else:
                p_init = 9.2
            p = _tf.get_variable("p_{}_{}".format(*t12), dtype = precision,
                initializer = _tf.constant(p_init, dtype = precision),
                trainable = pair_trainable,
                collections = [_tf.GraphKeys.MODEL_VARIABLES,
                                _tf.GraphKeys.GLOBAL_VARIABLES])
            _tf.summary.scalar("p_{}_{}".format(*t12), p, family = "modelParams")

        with _tf.variable_scope("{}_{}_r0".format(*t12), reuse = _tf.AUTO_REUSE):
            if ("r0", t12[0], t12[1]) in initial_params:
                r0_init = initial_params[("r0", t12[0], t12[1])]
            else:
                r0_init = 2.7
            r0 = _tf.get_variable("r0_{}_{}".format(*t12), dtype = precision,
                initializer = _tf.constant(r0_init, dtype = precision),
                trainable = r0_trainable,
                collections = [_tf.GraphKeys.MODEL_VARIABLES,
                                _tf.GraphKeys.GLOBAL_VARIABLES])
            if r0_trainable:
                _tf.summary.scalar("r0_{}_{}".format(*t12), r0,
                    family = "modelParams")

        pairPot = lambda r: A * _tf.exp(
            -p*(r/r0 - 1)) * poly_cutoff(r, cut_a, cut_b)
        return A, p, r0, pairPot

    @staticmethod
    def build_rho(t12, initial_params, cut_a, cut_b, r0_trainable, rho_trainable):
        """ builds the rho contribution for a given atom type tuple
            t12 = (t1, t2)
            returns: parameters xi, q, r0 and the corresponding rho contribution
                    as a function of r
        """
        with _tf.variable_scope("{}_{}_rho".format(*t12), reuse = _tf.AUTO_REUSE):
            if ("xi", t12[0], t12[1]) in initial_params:
                xi_init = initial_params[("xi", t12[0], t12[1])]
            else:
                xi_init = 1.6
            xi = _tf.get_variable("xi_{}_{}".format(*t12), dtype = precision,
                initializer = _tf.constant(xi_init, dtype = precision),
                trainable = rho_trainable,
                collections = [_tf.GraphKeys.MODEL_VARIABLES,
                                _tf.GraphKeys.GLOBAL_VARIABLES])
            _tf.summary.scalar("xi_{}_{}".format(*t12), xi, family = "modelParams")

            if ("q", t12[0], t12[1]) in initial_params:
                q_init = initial_params[("q", t12[0], t12[1])]
            else:
                q_init = 3.5
            q = _tf.get_variable("q_{}_{}".format(*t12), dtype = precision,
                initializer = _tf.constant(q_init, dtype = precision),
                trainable = rho_trainable,
                collections = [_tf.GraphKeys.MODEL_VARIABLES,
                                _tf.GraphKeys.GLOBAL_VARIABLES])
            _tf.summary.scalar("q_{}_{}".format(*t12), q,
                family = "modelParams")
        with _tf.variable_scope("{}_{}_r0".format(*t12), reuse = _tf.AUTO_REUSE):
            if ("r0", t12[0], t12[1]) in initial_params:
                r0_init = initial_params[("r0", t12[0], t12[1])]
            else:
                r0_init = 2.7
            r0 = _tf.get_variable("r0_{}_{}".format(*t12), dtype = precision,
                initializer = _tf.constant(r0_init, dtype = precision),
                trainable = r0_trainable,
                collections = [_tf.GraphKeys.MODEL_VARIABLES,
                                _tf.GraphKeys.GLOBAL_VARIABLES])
            if r0_trainable:
                _tf.summary.scalar("r0_{}_{}".format(*t12), r0,
                    family = "modelParams")

        rho = lambda r: xi**2 * _tf.exp(
            -2.0*q*(r/r0 - 1)) * poly_cutoff(r, cut_a, cut_b)
        return xi, q, r0, rho

class NNEpotential(EAMpotential):
    def __init__(self, atom_types, layers = [20], initial_params = None, cut_a = 5.4, cut_b = 8.1,
        r0_trainable = False, pair_trainable = True, rho_trainable = True, offsets = None):
        with _tf.variable_scope("NNEpot"):
            EAMpotential.__init__(self, atom_types)

            A = {}
            xi = {}
            p = {}
            q = {}
            r0 = {}
            pairPot = {}
            rho = {}

            for t1, t2 in combinations_with_replacement(atom_types, r = 2):
                t12 = tuple(sorted([t1, t2]))
                A[t12], p[t12], r0[t12], pairPot[t12] = SMATBpotential.build_pairPot(
                    t12, initial_params, cut_a, cut_b, r0_trainable, pair_trainable)
                xi[t12], q[t12], _, rho[t12] = SMATBpotential.build_rho(
                    t12, initial_params, cut_a, cut_b, r0_trainable, rho_trainable)

            if offsets == None:
                offsets = [0.0]*len(self.atom_types)

            for t1, offset in zip(self.atom_types, offsets):
                with _tf.variable_scope("{}_ANN".format(t1), reuse = _tf.AUTO_REUSE):
                    self.ANNs[t1] = EAMAtomicNN(atom_types, offset)
                    self.ANNs[t1].F = NNEpotential.build_F(
                        t1, initial_params, layers)

                    for t2 in self.atom_types:
                        t12 = tuple(sorted([t1, t2]))
                        with _tf.name_scope("{}_{}_PairPot".format(*t12)):
                            self.ANNs[t1].pairPot[t2] = pairPot[t12](
                                self.ANNs[t1].inputs[t2])
                        with _tf.name_scope("{}_{}_rho".format(*t12)):
                            self.ANNs[t1].rho[t2] = rho[t12](
                                self.ANNs[t1].inputs[t2])

                    self.ANNs[t1]._post_setup()
            EAMpotential._post_setup(self)

    @staticmethod
    def build_F(t1, initial_params, layers):
        with _tf.variable_scope("%s_EmbeddingFunc"%t1, reuse = _tf.AUTO_REUSE):
            if ("mu_std", t1) in initial_params:
                mu_init, std_init = initial_params[("mu_std", t1)]
            else:
                print("Using default parameters for NN norm")
                mu_init, std_init = -30, 15
            mu = _tf.get_variable("mu_rho_{}".format(t1), dtype = precision,
                initializer = _tf.constant(mu_init, dtype = precision),
                trainable = False,
                collections = [_tf.GraphKeys.MODEL_VARIABLES,
                                _tf.GraphKeys.GLOBAL_VARIABLES])
            std = _tf.get_variable("std_rho_{}".format(t1), dtype = precision,
                initializer = _tf.constant(std_init, dtype = precision),
                trainable = False,
                collections = [_tf.GraphKeys.MODEL_VARIABLES,
                                _tf.GraphKeys.GLOBAL_VARIABLES])
        def F(rho):
            for i, n in enumerate(layers):
                if i == 0:
                    layer, weights, bias = nn_layer((rho - mu)/std, 1, n,
                        name = "hiddenLayer_%d"%(i+1))
                else:
                    # Use previous layer as input
                    layer, weights, bias = nn_layer(layer,
                        layers[i-1], n, name = "hiddenLayer_%d"%(i+1))
            output, _, _ = nn_layer(layer, layers[-1], 1,
                act = None, name = "outputLayer")
            return -_tf.sqrt(rho)*output
        return F

class NNRHOpotential(EAMpotential):
    def __init__(self, atom_types, layers = [20],
        initial_params = None, r0_trainable = False, pair_trainable = True,
        cut_a = 5.4, cut_b = 8.1, offsets = None):
        with _tf.variable_scope("NNRHOpot", reuse = _tf.AUTO_REUSE):
            EAMpotential.__init__(self, atom_types)

            A = {}
            xi = {}
            p = {}
            q = {}
            r0 = {}
            pairPot = {}
            rho = {}

            for t1, t2 in combinations_with_replacement(atom_types, r = 2):
                t12 = tuple(sorted([t1, t2]))
                A[t12], p[t12], r0[t12], pairPot[t12] = SMATBpotential.build_pairPot(
                    t12, initial_params, cut_a, cut_b, r0_trainable, pair_trainable)

                rho[t12] = NNRHOpotential.build_rho(t12, initial_params,
                    layers, cut_a, cut_b, r0_trainable)

            if offsets == None:
                offsets = [0.0]*len(self.atom_types)

            for t1, offset in zip(self.atom_types, offsets):
                with _tf.name_scope("{}_ANN".format(t1)):
                    self.ANNs[t1] = EAMAtomicNN(atom_types, offset, "%s"%t1)
                    self.ANNs[t1].F = lambda rho: -_tf.sqrt(rho)

                    for t2 in self.atom_types:
                        t12 = tuple(sorted([t1, t2]))
                        with _tf.variable_scope("{}_{}_PairPot".format(*t12), reuse = _tf.AUTO_REUSE):
                            self.ANNs[t1].pairPot[t2] = pairPot[t12](
                                self.ANNs[t1].inputs[t2])
                        with _tf.variable_scope("{}_{}_rho".format(*t12), reuse = _tf.AUTO_REUSE):
                            self.ANNs[t1].rho[t2] = rho[t12](
                                self.ANNs[t1].inputs[t2])

                    self.ANNs[t1]._post_setup()
            EAMpotential._post_setup(self)

    @staticmethod
    def build_rho(t12, initial_params, layers, cut_a, cut_b, r0_trainable):
        with _tf.variable_scope("{}_{}_r0".format(*t12), reuse = _tf.AUTO_REUSE):
            if ("r0", t12[0], t12[1]) in initial_params:
                r0_init = initial_params[("r0", t12[0], t12[1])]
            else:
                r0_init = 2.7
            r0 = _tf.get_variable("r0_{}_{}".format(*t12), dtype = precision,
                initializer = _tf.constant(r0_init, dtype = precision),
                trainable = r0_trainable,
                collections = [_tf.GraphKeys.MODEL_VARIABLES,
                                _tf.GraphKeys.GLOBAL_VARIABLES])
            if r0_trainable:
                _tf.summary.scalar("r0_{}_{}".format(*t12), r0,
                    family = "modelParams")

        def rho(r):
            for i, n in enumerate(layers):
                if i == 0:
                    layer, weights, bias = nn_layer(r/r0-1, 1, n,
                        name = "hiddenLayer_%d"%(i+1))
                else:
                    # Use previous layer as input
                    layer, weights, bias = nn_layer(layer,
                        layers[i-1], n, name = "hiddenLayer_%d"%(i+1))
            output, _, _ = nn_layer(layer, layers[-1], 1,
                act = None, name = "outputLayer")
            return _tf.exp(output) * poly_cutoff(r, cut_a, cut_b)
        return rho

class NNVRHOpotential(EAMpotential):
    def __init__(self, atom_types, layers_V = [20], layers_rho = [20],
        initial_params = None, r0_trainable = False, cut_a = 5.4, cut_b = 8.1,
        offsets = None):
        with _tf.variable_scope("NNVRHOpot", reuse = _tf.AUTO_REUSE):
            EAMpotential.__init__(self, atom_types)

            r0 = {}
            pairPot = {}
            rho = {}

            for t1, t2 in combinations_with_replacement(atom_types, r = 2):
                t12 = tuple(sorted([t1, t2]))
                pairPot[t12] = NNVRHOpotential.build_pairPot(t12,
                    initial_params, layers_V, cut_a, cut_b, r0_trainable)

                rho[t12] = NNRHOpotential.build_rho(t12, initial_params,
                    layers_rho, cut_a, cut_b, r0_trainable)

            if offsets == None:
                offsets = [0.0]*len(self.atom_types)

            for t1, offset in zip(self.atom_types, offsets):
                with _tf.name_scope("{}_ANN".format(t1)):
                    self.ANNs[t1] = EAMAtomicNN(atom_types, offset, "%s"%t1)
                    self.ANNs[t1].F = lambda rho: -_tf.sqrt(rho)

                    for t2 in self.atom_types:
                        t12 = tuple(sorted([t1, t2]))
                        with _tf.variable_scope("{}_{}_PairPot".format(*t12), reuse = _tf.AUTO_REUSE):
                            self.ANNs[t1].pairPot[t2] = pairPot[t12](
                                self.ANNs[t1].inputs[t2])
                        with _tf.variable_scope("{}_{}_rho".format(*t12), reuse = _tf.AUTO_REUSE):
                            self.ANNs[t1].rho[t2] = rho[t12](
                                self.ANNs[t1].inputs[t2])

                    self.ANNs[t1]._post_setup()
            EAMpotential._post_setup(self)

    @staticmethod
    def build_pairPot(t12, initial_params, layers, cut_a, cut_b, r0_trainable):
        with _tf.variable_scope("{}_{}_r0".format(*t12), reuse = _tf.AUTO_REUSE):
            if ("r0", t12[0], t12[1]) in initial_params:
                r0_init = initial_params[("r0", t12[0], t12[1])]
            else:
                r0_init = 2.7
            r0 = _tf.get_variable("r0_{}_{}".format(*t12), dtype = precision,
                initializer = _tf.constant(r0_init, dtype = precision),
                trainable = r0_trainable,
                collections = [_tf.GraphKeys.MODEL_VARIABLES,
                                _tf.GraphKeys.GLOBAL_VARIABLES])
            if r0_trainable:
                _tf.summary.scalar("r0_{}_{}".format(*t12), r0,
                    family = "modelParams")

        def V(r):
            for i, n in enumerate(layers):
                if i == 0:
                    layer, weights, bias = nn_layer(r/r0-1, 1, n,
                        name = "hiddenLayer_%d"%(i+1))
                else:
                    # Use previous layer as input
                    layer, weights, bias = nn_layer(layer,
                        layers[i-1], n, name = "hiddenLayer_%d"%(i+1))
            output, _, _ = nn_layer(layer, layers[-1], 1,
                act = None, name = "outputLayer")
            return _tf.exp(output) * poly_cutoff(r, cut_a, cut_b)
        return V

class NNERHOpotential(EAMpotential):
    def __init__(self, atom_types, layers_F = [20], layers_rho = [20],
        initial_params = None, r0_trainable = False, pair_trainable = True,
        cut_a = 5.4, cut_b = 8.1, offsets = None):
        with _tf.variable_scope("NNERHOpot", reuse = _tf.AUTO_REUSE):
            EAMpotential.__init__(self, atom_types)

            A = {}
            xi = {}
            p = {}
            q = {}
            r0 = {}
            pairPot = {}
            rho = {}

            for t1, t2 in combinations_with_replacement(atom_types, r = 2):
                t12 = tuple(sorted([t1, t2]))
                A[t12], p[t12], r0[t12], pairPot[t12] = SMATBpotential.build_pairPot(
                    t12, initial_params, cut_a, cut_b, r0_trainable, pair_trainable)

                rho[t12] = NNRHOpotential.build_rho(t12, initial_params,
                    layers_rho, cut_a, cut_b, r0_trainable)

            if offsets == None:
                offsets = [0.0]*len(self.atom_types)

            for t1, offset in zip(self.atom_types, offsets):
                with _tf.name_scope("{}_ANN".format(t1)):
                    self.ANNs[t1] = EAMAtomicNN(atom_types, offset, "%s"%t1)
                    self.ANNs[t1].F = NNEpotential.build_F(
                        t1, initial_params, layers_F)

                    for t2 in self.atom_types:
                        t12 = tuple(sorted([t1, t2]))
                        with _tf.variable_scope("{}_{}_PairPot".format(*t12), reuse = _tf.AUTO_REUSE):
                            self.ANNs[t1].pairPot[t2] = pairPot[t12](
                                self.ANNs[t1].inputs[t2])
                        with _tf.variable_scope("{}_{}_rho".format(*t12), reuse = _tf.AUTO_REUSE):
                            self.ANNs[t1].rho[t2] = rho[t12](
                                self.ANNs[t1].inputs[t2])

                    self.ANNs[t1]._post_setup()
            EAMpotential._post_setup(self)

class NNVERHOpotential(EAMpotential):
    def __init__(self, atom_types, layers_F = [20], layers_V = [20], layers_rho = [20],
        initial_params = None, r0_trainable = False, cut_a = 5.4, cut_b = 8.1,
        offsets = None):
        with _tf.variable_scope("NNVERHOpot", reuse = _tf.AUTO_REUSE):
            EAMpotential.__init__(self, atom_types)

            r0 = {}
            pairPot = {}
            rho = {}

            for t1, t2 in combinations_with_replacement(atom_types, r = 2):
                t12 = tuple(sorted([t1, t2]))
                pairPot[t12] = NNVERHOpotential.build_pairPot(t12,
                    initial_params, layers_V, cut_a, cut_b, r0_trainable)

                rho[t12] = NNRHOpotential.build_rho(t12, initial_params,
                    layers_rho, cut_a, cut_b, r0_trainable)

            if offsets == None:
                offsets = [0.0]*len(self.atom_types)

            for t1, offset in zip(self.atom_types, offsets):
                with _tf.name_scope("{}_ANN".format(t1)):
                    self.ANNs[t1] = EAMAtomicNN(atom_types, offset, "%s"%t1)
                    self.ANNs[t1].F = NNEpotential.build_F(
                        t1, initial_params, layers_F)

                    for t2 in self.atom_types:
                        t12 = tuple(sorted([t1, t2]))
                        with _tf.variable_scope("{}_{}_PairPot".format(*t12), reuse = _tf.AUTO_REUSE):
                            self.ANNs[t1].pairPot[t2] = pairPot[t12](
                                self.ANNs[t1].inputs[t2])
                        with _tf.variable_scope("{}_{}_rho".format(*t12), reuse = _tf.AUTO_REUSE):
                            self.ANNs[t1].rho[t2] = rho[t12](
                                self.ANNs[t1].inputs[t2])

                    self.ANNs[t1]._post_setup()
            EAMpotential._post_setup(self)

    @staticmethod
    def build_pairPot(t12, initial_params, layers, cut_a, cut_b, r0_trainable):
        with _tf.variable_scope("{}_{}_PairPot".format(*t12), reuse = _tf.AUTO_REUSE):
            if ("A", t12[0], t12[1]) in initial_params:
                A_init = initial_params[("A", t12[0], t12[1])]
            else:
                A_init = 0.2
            A = _tf.get_variable("A_{}_{}".format(*t12), dtype = precision,
                initializer = _tf.constant(A_init, dtype = precision),
                collections = [_tf.GraphKeys.MODEL_VARIABLES,
                                _tf.GraphKeys.GLOBAL_VARIABLES])
            _tf.summary.scalar("A_{}_{}".format(*t12), A, family = "modelParams")

        with _tf.variable_scope("{}_{}_r0".format(*t12), reuse = _tf.AUTO_REUSE):
            if ("r0", t12[0], t12[1]) in initial_params:
                r0_init = initial_params[("r0", t12[0], t12[1])]
            else:
                r0_init = 2.7
            r0 = _tf.get_variable("r0_{}_{}".format(*t12), dtype = precision,
                initializer = _tf.constant(r0_init, dtype = precision),
                trainable = r0_trainable,
                collections = [_tf.GraphKeys.MODEL_VARIABLES,
                                _tf.GraphKeys.GLOBAL_VARIABLES])
            if r0_trainable:
                _tf.summary.scalar("r0_{}_{}".format(*t12), r0,
                    family = "modelParams")

        def V(r):
            for i, n in enumerate(layers):
                if i == 0:
                    layer, weights, bias = nn_layer(r/r0-1, 1, n,
                        name = "hiddenLayer_%d"%(i+1))
                else:
                    # Use previous layer as input
                    layer, weights, bias = nn_layer(layer,
                        layers[i-1], n, name = "hiddenLayer_%d"%(i+1))
            output, _, _ = nn_layer(layer, layers[-1], 1,
                act = None, name = "outputLayer")
            return A*((1.0 - _tf.exp(output))**2 - 1) * poly_cutoff(r, cut_a, cut_b)
        return V

class NNfreeERHOpotential(EAMpotential):
    def __init__(self, atom_types, layers_F = [20], layers_rho = [20],
        initial_params = None, r0_trainable = False, pair_trainable = True,
        cut_a = 5.4, cut_b = 8.1, offsets = None):
        with _tf.variable_scope("NNfreeERHOpot", reuse = _tf.AUTO_REUSE):
            EAMpotential.__init__(self, atom_types)

            A = {}
            xi = {}
            p = {}
            q = {}
            r0 = {}
            pairPot = {}
            rho = {}

            for t1, t2 in combinations_with_replacement(atom_types, r = 2):
                t12 = tuple(sorted([t1, t2]))
                A[t12], p[t12], r0[t12], pairPot[t12] = SMATBpotential.build_pairPot(
                    t12, initial_params, cut_a, cut_b, r0_trainable, pair_trainable)

                rho[t12] = NNfreeERHOpotential.build_rho(t12, initial_params,
                    layers_rho, cut_a, cut_b, r0_trainable)

            if offsets == None:
                offsets = [0.0]*len(self.atom_types)

            for t1, offset in zip(self.atom_types, offsets):
                with _tf.name_scope("{}_ANN".format(t1)):
                    self.ANNs[t1] = EAMAtomicNN(atom_types, offset, "%s"%t1)
                    self.ANNs[t1].F = NNfreeERHOpotential.build_F(
                        t1, initial_params, layers_F)

                    for t2 in self.atom_types:
                        t12 = tuple(sorted([t1, t2]))
                        with _tf.variable_scope("{}_{}_PairPot".format(*t12), reuse = _tf.AUTO_REUSE):
                            self.ANNs[t1].pairPot[t2] = pairPot[t12](
                                self.ANNs[t1].inputs[t2])
                        with _tf.variable_scope("{}_{}_rho".format(*t12), reuse = _tf.AUTO_REUSE):
                            self.ANNs[t1].rho[t2] = rho[t12](
                                self.ANNs[t1].inputs[t2])

                    self.ANNs[t1]._post_setup()
            EAMpotential._post_setup(self)

    @staticmethod
    def build_rho(t12, initial_params, layers, cut_a, cut_b, r0_trainable):
        with _tf.variable_scope("{}_{}_r0".format(*t12), reuse = _tf.AUTO_REUSE):
            if ("r0", t12[0], t12[1]) in initial_params:
                r0_init = initial_params[("r0", t12[0], t12[1])]
            else:
                r0_init = 2.7
            r0 = _tf.get_variable("r0_{}_{}".format(*t12), dtype = precision,
                initializer = _tf.constant(r0_init, dtype = precision),
                trainable = r0_trainable,
                collections = [_tf.GraphKeys.MODEL_VARIABLES,
                                _tf.GraphKeys.GLOBAL_VARIABLES])
            if r0_trainable:
                _tf.summary.scalar("r0_{}_{}".format(*t12), r0,
                    family = "modelParams")

        def rho(r):
            for i, n in enumerate(layers):
                if i == 0:
                    layer, weights, bias = nn_layer(r/r0-1, 1, n,
                        name = "hiddenLayer_%d"%(i+1))
                else:
                    # Use previous layer as input
                    layer, weights, bias = nn_layer(layer,
                        layers[i-1], n, name = "hiddenLayer_%d"%(i+1))
            output, _, _ = nn_layer(layer, layers[-1], 1,
                act = None, name = "outputLayer")
            return output * poly_cutoff(r, cut_a, cut_b)
        return rho

    @staticmethod
    def build_F(t1, initial_params, layers):
        with _tf.variable_scope("%s_EmbeddingFunc"%t1, reuse = _tf.AUTO_REUSE):
            if ("mu_std", t1) in initial_params:
                mu_init, std_init = initial_params[("mu_std", t1)]
            else:
                print("Using default parameters for NN norm")
                mu_init, std_init = -30, 15
            mu = _tf.get_variable("mu_rho_{}".format(t1), dtype = precision,
                initializer = _tf.constant(mu_init, dtype = precision),
                trainable = False,
                collections = [_tf.GraphKeys.MODEL_VARIABLES,
                                _tf.GraphKeys.GLOBAL_VARIABLES])
            std = _tf.get_variable("std_rho_{}".format(t1), dtype = precision,
                initializer = _tf.constant(std_init, dtype = precision),
                trainable = False,
                collections = [_tf.GraphKeys.MODEL_VARIABLES,
                                _tf.GraphKeys.GLOBAL_VARIABLES])
        def F(rho):
            for i, n in enumerate(layers):
                if i == 0:
                    layer, weights, bias = nn_layer((rho - mu)/std, 1, n,
                        name = "hiddenLayer_%d"%(i+1))
                else:
                    # Use previous layer as input
                    layer, weights, bias = nn_layer(layer,
                        layers[i-1], n, name = "hiddenLayer_%d"%(i+1))
            output, _, _ = nn_layer(layer, layers[-1], 1,
                act = None, name = "outputLayer")
            return output
        return F
